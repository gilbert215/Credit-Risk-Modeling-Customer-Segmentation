{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gilbert215/Credit-Risk-Modeling-Customer-Segmentation/blob/main/114_611_In_Class_Exercise_(Nov_6).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 114/611 In-class exercise: Prompt Engineering for QA\n",
        "\n",
        "In this exercise, you will construct specific prompts for a set of expected answer types, and learn about different evaluation methods.\n",
        "\n",
        "We will use `gpt-4o-mini-2024-07-18`, so be sure you have an OpenAI key, which you may create on the [CMU AI Gateway](https://ai-gateway.andrew.cmu.edu/ui/?login=success&page=api-keys).\n",
        "\n",
        "*(**Warning**: Each student has $50 API credit throughout the semester, so please keep track of your usage!)*\n",
        "\n",
        "Please submit your notebook in a completed state, i.e. run the notebook to completion, and don't erase the content of the output cells, especially in the \"üìã**Results**\" sections. Remember to answer the \"‚úçÔ∏è**Reflection**\" sections in the notebook!\n",
        "\n"
      ],
      "metadata": {
        "id": "Ewk0qV_Tmhrv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge of QA: How do we evaluate the answers?"
      ],
      "metadata": {
        "id": "XFS9cpoe2N90"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When humans ask and answer questions, we rarely stick to strict formats or predefined options like in multiple-choice tests. Instead, we interpret the information we receive and decide whether it makes sense or satisfies our intent.\n",
        "\n",
        "Dealing QA systems is more difficult. Even for factual questions, it can be difficult to assess the answers when no clear constraints or examples are provided, where methods like exact match may fail. This is more complicated for questions that require free-form generation.\n",
        "\n",
        "In this notebook, we focus on factual QA, and in this section, we'll explore some evaluation methods based on the example below."
      ],
      "metadata": {
        "id": "4IPnVxKb21jP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE THIS CELL\n",
        "question = \"What are the benefits of exercise?\"\n",
        "expectedAnswer = [\n",
        "    \"improves heart health\",\n",
        "    \"strengthens muscles\",\n",
        "    \"boosts mental health\"\n",
        "]\n",
        "possible_answers = {\n",
        "    \"Answer 1\": \"Exercise improves heart health, strengthens muscles, and boosts mental health.\",\n",
        "    \"Answer 2\": \"Exercise improves heart health but often harms mental health due to stress.\",\n",
        "    \"Answer 3\": \"Exercise keeps the body strong and improves mood.\",\n",
        "    \"Answer 4\": \"Exercise is tiring, weakens the body, and causes stress.\",\n",
        "}"
      ],
      "metadata": {
        "id": "V9mOwDfxJCym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method 1 : Soft Match (Factoid and Factoid List Answers)\n",
        "\n"
      ],
      "metadata": {
        "id": "R5PRxjCufYD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Soft-match** is a string-matching-based evaluation method that relaxes the constraint of exact match. It can also handle cases where multiple answers are acceptable.\n",
        "\n",
        "For a single expected answer (**factoid**), we check whether the expected answer appears in the model's output.\n",
        "- If it does not occur, the score is 0.\n",
        "- If it does, the score is computed as the ratio of the number of characters in the expected answer to the number of characters in the generated answer, which penalizes redundant words.\n",
        "\n",
        "For multiple expected answers (**factoid list**), we assume the generated output is a comma-separated list of distinct factoids. In this case, we sum the scores for each expected factoid, and apply a discount factor to account for the extra punctuation or separators in the list."
      ],
      "metadata": {
        "id": "jM0bWSc7rO4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE THIS CELL\n",
        "import string\n",
        "\n",
        "def softMatch ( generatedString , expectedString, discount ):\n",
        "  if expectedString in generatedString:\n",
        "    score = len(expectedString) / (len(generatedString) - discount)\n",
        "  else:\n",
        "    score = 0;\n",
        "  return score\n",
        "\n",
        "def evaluate_soft_match(question, answer, expectedAnswer):\n",
        "    if isinstance(expectedAnswer, list):\n",
        "        total = 0\n",
        "        discount = 2*len(expectedAnswer)\n",
        "        for exp in expectedAnswer:\n",
        "            total += softMatch(answer, exp, discount)\n",
        "        print(\"\\nSoftmatch Score:\", f\"{total:.2f}\")\n",
        "    else:\n",
        "        score = softMatch(answer, expectedAnswer, 0)\n",
        "        print(\"\\nSoftmatch Score:\", f\"{score:.2f}\")\n"
      ],
      "metadata": {
        "id": "pMFd2TezeDKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE THIS CELL\n",
        "# Soft Match Exploration Cell\n",
        "print(f\"Question: {question}\\n\")\n",
        "for label, ans in possible_answers.items():\n",
        "    print(f\"--- {label} ---\")\n",
        "    evaluate_soft_match(question, ans, expectedAnswer)\n",
        "    print(f\"Answer: {ans}\\n\")\n"
      ],
      "metadata": {
        "id": "mMJuvXxVgU7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚úçÔ∏è **Reflection**\n",
        "\n",
        "Please use the text cell below to answer the following question:\n",
        "\n",
        "What are the strengths and weaknesses of SoftMatch? Give at least one strength and one weakness; refer to the examples above for in order to illustrate your points."
      ],
      "metadata": {
        "id": "air0b26IiV1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your Answer**\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "gCYLjxNbiV1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method 2: LLM as a Judge"
      ],
      "metadata": {
        "id": "7FG7FRiggF-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to evaluate answers is by using a **LLM as a judge**, which is a popular method recently. In real-world applications, we often choose the best affordable LLM available to serve as the evaluator.\n",
        "\n",
        "To make LLMs judge, we have to provide explicit evaluation instructions to the model. In this notebook, we ask the LLM to rate each answer on a Likert scale from 0 to 5, and then normalize the score to a 0-1 range so it can be compared directly with the soft-match score.\n",
        "\n",
        "We also track the evaluation cost based on the data stored in the `PRICE` dictionary."
      ],
      "metadata": {
        "id": "tQ9W9eZfEHUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define prompts and price of the model we choose\n",
        "\n",
        "JUDGE_PROMPT = \"\"\"# Instruction\n",
        "You will be given a question, gold answer, and system answer.\n",
        "Your task is to provide a 'total rating' scoring\n",
        "how well the system answer matches the gold answer for the question.\n",
        "Give your answer as an integer on a scale of 0 to 5, where\n",
        "0 means that the system answer does not match the gold answer at all,\n",
        "and 5 means that the system answer matches the gold answer.\n",
        "\n",
        "Provide your feedback as follows:\n",
        "\n",
        "# Feedback\n",
        "Rational: (your thinking process)\n",
        "Total rating: (your rating, as an integer from 0 to 5)\"\"\"\n",
        "\n",
        "TASK_PROMPT = \"\"\"# Task\n",
        "Now here are the question and answer.\n",
        "Question: {question}\n",
        "Gold Answer: {gold_answer}\n",
        "System Answer: {system_answer}\n",
        "\n",
        "# Feedback\n",
        "Rational: \"\"\"\n",
        "\n",
        "PRICE = {\n",
        "    'input_tokens': 0.15/1e6,\n",
        "    'output_tokens': 0.60/1e6\n",
        "}"
      ],
      "metadata": {
        "id": "_hMQqqzQJUr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE THIS CELL\n",
        "from collections import defaultdict\n",
        "\n",
        "def parse(text):\n",
        "    \"\"\"\n",
        "    Parse an output, assuming the following output format:\n",
        "    xxx Total rating: y\n",
        "\n",
        "    \"\"\"\n",
        "    output = 0\n",
        "    if 'Total rating:' in text:\n",
        "        splits = text.split('Total rating:')\n",
        "        score = splits[-1].strip()\n",
        "        if score.isdigit():\n",
        "            output = int(score)\n",
        "        else:\n",
        "            print(f\"Error: score cannot be converted to integer.\")\n",
        "    else:\n",
        "        print(f\"Error: output does not follow the specified format.\")\n",
        "\n",
        "    return output\n",
        "\n",
        "def evaluate_llm_as_a_judge(judge, examples: list[dict[str, str]]):\n",
        "    \"\"\"\n",
        "    Given a judge and examples, print out\n",
        "    * the average score\n",
        "    * the api cost for the evaluation\n",
        "    and return the score scaled to 0-1\n",
        "\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    usage = defaultdict(int)\n",
        "    for example in examples:\n",
        "\n",
        "        response = judge.responses.create(\n",
        "            model=\"gpt-4o-mini-2024-07-18\",\n",
        "            instructions=JUDGE_PROMPT,\n",
        "            input=TASK_PROMPT.format(\n",
        "                question=example['question'],\n",
        "                gold_answer=example['gold_answer'],\n",
        "                system_answer=example['system_answer']\n",
        "                )\n",
        "        )\n",
        "        output_raw = response.output[0].content[0].text\n",
        "        scores.append(parse(output_raw))\n",
        "        usage['input_tokens'] += response.usage.input_tokens\n",
        "        usage['output_tokens'] += response.usage.output_tokens\n",
        "\n",
        "    cost = sum(usage[k]*v for k, v in PRICE.items())\n",
        "\n",
        "    avg = sum(scores)/len(scores) / 5 # normalize\n",
        "    print(f\"\\nLLM Likert Score (normalized): {avg:.2f} (Cost: {cost:.4f} USD)\")\n",
        "    return\n"
      ],
      "metadata": {
        "id": "nkM8Vi4O93J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE THIS CELL\n",
        "# Get the user's OpenAI key and create a client model to be used for answering\n",
        "# questions as well as judging answers.\n",
        "import openai\n",
        "import getpass\n",
        "llm = openai.OpenAI(\n",
        "    api_key=getpass.getpass(\"Enter your OpenAI API key for gpt-4o-mini:\"),\n",
        "    base_url=\"https://ai-gateway.andrew.cmu.edu/\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6AwALLxzc0r",
        "outputId": "770db1ea-2153-4ce9-97c5-17c8627d1054"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key for gpt-4o-mini:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE THIS CELL\n",
        "# LLM as a judge Exploration Cell\n",
        "print(f\"Question: {question}\\n\")\n",
        "gold_answer = \", \".join(expectedAnswer)\n",
        "for label, ans in possible_answers.items():\n",
        "  print(f\"--- {label} ---\")\n",
        "  evaluate_llm_as_a_judge(llm, [{\"question\": question, \"system_answer\": ans, \"gold_answer\": gold_answer}])\n",
        "  print(f\"Answer: {ans}\\n\")"
      ],
      "metadata": {
        "id": "1VPLTApdj1TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚úçÔ∏è **Reflection**\n",
        "\n",
        "Please use the text cell below to answer the following question:\n",
        "\n",
        "Comparing to SoftMatch, how is the performance of LLM as a judge? Give at least one strength and one weakness; refer to the examples above in order to illustrate your points."
      ],
      "metadata": {
        "id": "8BNTemp4zmGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your Answer**\n",
        "\n",
        "...\n",
        "\n"
      ],
      "metadata": {
        "id": "8H_VJEnOz7N0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hands-on: How do we get better answers from LLMs?"
      ],
      "metadata": {
        "id": "2qEShTTyMqRs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QA Data"
      ],
      "metadata": {
        "id": "J3A9LVIpfnk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the question and answer pairs, while `type` indicates the format of the expected answer.\n",
        "Therefore, some questions are included more than once, with different `type` and `answer`."
      ],
      "metadata": {
        "id": "oHLZZfLpnSRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE THIS CELL.\n",
        "\n",
        "q1 = {\n",
        "    \"type\": \"LAST_NAME\",\n",
        "    \"question\": \"Who are the last three presidents of Carnegie Mellon?\",\n",
        "    \"answer\": [\"Suresh\", \"Cohon\", \"Mehrabian\"],\n",
        "}\n",
        "q2 = {\n",
        "    \"type\": \"FULL_NAME\",\n",
        "    \"question\": \"Who are the last three presidents of Carnegie Mellon?\",\n",
        "    \"answer\": [\"Subra Suresh\", \"Jared L. Cohon\", \"Robert Mehrabian\"],\n",
        "}\n",
        "q3 = {\n",
        "    \"type\": \"FIRST_NAME\",\n",
        "    \"question\": \"Who are the last three presidents of Carnegie Mellon?\",\n",
        "    \"answer\": [\"Subra\", \"Jared\", \"Robert\"],\n",
        "}\n",
        "q4 = {\n",
        "    \"type\": \"LAST_NAME\",\n",
        "    \"question\": \"Who is the current president of Carnegie Mellon?\",\n",
        "    \"answer\": \"Jahanian\"\n",
        "}\n",
        "q5 = {\n",
        "    \"type\": \"FULL_NAME\",\n",
        "    \"question\": \"Who is the current president of Carnegie Mellon?\",\n",
        "    \"answer\": \"Farnam Jahanian\"\n",
        "}\n",
        "q6 = {\n",
        "    \"type\": \"FIRST_NAME\",\n",
        "    \"question\": \"Who is the current president of Carnegie Mellon?\",\n",
        "    \"answer\": \"Farnam\"\n",
        "}\n",
        "q7 = {\n",
        "    \"type\": \"MONTH\",\n",
        "    \"question\": \"When was Charles Dickens born?\",\n",
        "    \"answer\": \"February\"\n",
        "}\n",
        "q8 = {\n",
        "    \"type\": \"DATE\",\n",
        "    \"question\": \"When was Charles Dickens born?\",\n",
        "    \"answer\": \"February 7, 1812\"\n",
        "}\n",
        "q9 = {\n",
        "    \"type\": \"LIST_OF_STEPS\",\n",
        "    \"question\": \"How do I get a PA driver's license?\",\n",
        "    \"answer\": [\"Get a medical exam\",\"Study the manual\",\"Gather required documents\",\"Take the knowledge and vision tests\",\"Receive your permit and practice\",\"Schedule and pass the road test\",\"Get your license at a Photo License Center\"]\n",
        "}\n",
        "\n",
        "q10 = {\n",
        "    \"type\": \"LIST_OF_STEPS\",\n",
        "    \"question\": \"How do I get a US passport?\",\n",
        "    \"answer\": [\"Complete and print Form DS-11\", \"Gather proof of U.S. citizenship and identity\" , \"Get a passport photo\", \"Make an appointment at an acceptance facility to submit your application in person\"]\n",
        "}"
      ],
      "metadata": {
        "id": "Zq3bb19JhVpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answering question with LLMs"
      ],
      "metadata": {
        "id": "Dw3AgY4-f_PF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the data and the LLM API ready, let's start answering questions using the basic setup!\n",
        "\n",
        "In the `answerQuestion` function, you can choose which **question**, **prompt format**, and **evaluation method** to use in order to test how the LLM performs.\n",
        "In the basic setup, where `type_specific_prompts` is empty, all question types use the general prompt description.\n",
        "The full prompt and response are then generated using `llm.chat.completions.create`, and then answer is evaluated automatically.\n",
        "\n",
        "**Note**: Subsequent calls to the model with the same prompt and question may produce slightly different outputs. Don‚Äôt worry ‚Äî you can report a single representative result in the output cells below."
      ],
      "metadata": {
        "id": "WzebtuKZroDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE THIS CELL\n",
        "# Define the dict used to store prompts per answer type.\n",
        "type_specific_prompts = {}"
      ],
      "metadata": {
        "id": "-F8tljxSlIos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE THIS CELL\n",
        "\n",
        "def answerQuestion ( input , prompt_type=\"general\", evaluation_type=\"softmatch\"):\n",
        "  answerType = input[\"type\"]\n",
        "  question = input[\"question\"]\n",
        "  expectedAnswer = input[\"answer\"]\n",
        "  if prompt_type == \"general\":\n",
        "    prompt = \"Answer the following question.\"\n",
        "  else: # use type specific prompts\n",
        "    prompt = type_specific_prompts.get( answerType , \"Answer the following question.\")\n",
        "\n",
        "  print(\"Answer Type: \"+ answerType )\n",
        "  print(\"\\nPrompt: \" + prompt )\n",
        "  print(\"\\nQuestion: \" + question )\n",
        "\n",
        "  response = llm.chat.completions.create(\n",
        "    model=\"gpt-4o-mini-2024-07-18\",\n",
        "    messages = [\n",
        "        { \"role\": \"system\", \"content\": f\"{prompt}\" },\n",
        "        { \"role\": \"user\", \"content\":  f\"Question:\\n{question}\"}\n",
        "\n",
        "    ]\n",
        "  )\n",
        "  answer = response.choices[0].message.content\n",
        "  print(\"\\nAnswer:\\n\" + answer )\n",
        "  print(\"\\nExpected Answer:\\n\" + str(expectedAnswer) )\n",
        "\n",
        "  if evaluation_type == \"softmatch\":\n",
        "    evaluate_soft_match( question, answer, expectedAnswer )\n",
        "  else: # eval with llm as a judge\n",
        "    evaluate_llm_as_a_judge(llm, [{\"question\": question, \"system_answer\": answer, \"gold_answer\": expectedAnswer}])\n"
      ],
      "metadata": {
        "id": "804X7uzXEOez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try answering a single question like this!\n",
        "answerQuestion( q1 )"
      ],
      "metadata": {
        "id": "scVxKOAqXoxW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c1fb652-b631-4ecc-9a76-c5c3ffc34260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer Type: LAST_NAME\n",
            "\n",
            "Prompt: Answer the following question.\n",
            "\n",
            "Question: Who are the last three presidents of Carnegie Mellon?\n",
            "\n",
            "Answer:\n",
            "As of October 2023, the last three presidents of Carnegie Mellon University are:\n",
            "\n",
            "1. **Jared L. Cohon** (1997‚Äì2013)\n",
            "2. **Subra Suresh** (2013‚Äì2017)\n",
            "3. **Farnam Jahanian** (2017‚Äìpresent)\n",
            "\n",
            "Farnam Jahanian is the current president.\n",
            "\n",
            "Expected Answer:\n",
            "['Suresh', 'Cohon', 'Mehrabian']\n",
            "\n",
            "Softmatch Score: 0.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Results (to compare with the next section)"
      ],
      "metadata": {
        "id": "TStz_fh3XiZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change this cell if needed. The output should include the results of all questions.\n",
        "\n",
        "use_llm_as_a_judge = [] # TODO: Choose suitable evaluation type for each question.\n",
        "for i, question in enumerate([q1, q2, q3, q4, q5, q6, q7, q8, q9, q10]):\n",
        "  print(f\"---------------- Question {i+1} ----------------\")\n",
        "  if i in use_llm_as_a_judge:\n",
        "    answerQuestion( question , evaluation_type=\"llm_as_a_judge\" )\n",
        "  else:\n",
        "    answerQuestion( question )"
      ],
      "metadata": {
        "id": "w5BMxQHzV-CI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Engineering 1: Answer Format"
      ],
      "metadata": {
        "id": "R0x3tOEZUe51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After experimenting with `answerQuestion`, you may have noticed some limitations of the general prompt. For the next step, you'll tune the prompts to **produce answers in the correct format** for each question type!\n",
        "\n",
        "Your goal is to improve performance by **adjusting only the prompts below**. Try to design prompts so that the generated output is as close as possible to the expected answer, given what the LLM is capable of producing.\n",
        "\n",
        "*(You don't need to add RAG contexts or external information; the focus here is on using prompts to control the level of detail and structure in the output.)*\n",
        "\n",
        "**Hint**: Work through the questions one by one. After updating a prompt, re-run the corresponding evaluation cell to see how your new prompt affects the model's response."
      ],
      "metadata": {
        "id": "EnZPkAjPO6To"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Change the prompts!\n",
        "\n",
        "# Names\n",
        "type_specific_prompts[\"LAST_NAME\"] = \"Answer the given question.\"\n",
        "type_specific_prompts[\"FULL_NAME\"] = \"Answer the given question.\"\n",
        "type_specific_prompts[\"FIRST_NAME\"] = \"Answer the given question.\"\n",
        "\n",
        "# Dates\n",
        "type_specific_prompts[\"MONTH\"] = \"Answer the given question.\"\n",
        "type_specific_prompts[\"DATE\"] = \"Answer the given question.\"\n",
        "\n",
        "# Steps\n",
        "type_specific_prompts[\"LIST_OF_STEPS\"] = \"Answer the given question.\""
      ],
      "metadata": {
        "id": "H98ZvMNJcJqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try answering questions one by one! Index to format type:\n",
        "# LAST_NAME: q1, q4\n",
        "# FULL_NAME: q2, q5\n",
        "# FIRST_NAME: q3, q6\n",
        "# MONTH / DATE: q7, q8\n",
        "# LIST_OF_STEPS: q9, q10\n",
        "answerQuestion( q1, prompt_type=\"specific\")"
      ],
      "metadata": {
        "id": "8BuHAafvZahe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üìã**Results**"
      ],
      "metadata": {
        "id": "xxNstwzbZahf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change this cell if needed. The output should include the results of all questions.\n",
        "\n",
        "use_llm_as_a_judge = [] # TODO: Choose suitable evaluation type for each question.\n",
        "for i, question in enumerate([q1, q2, q3, q4, q5, q6, q7, q8, q9, q10]):\n",
        "  print(f\"---------------- Question {i+1} ----------------\")\n",
        "  if i in use_llm_as_a_judge:\n",
        "    answerQuestion( question , prompt_type=\"specific\" , evaluation_type=\"llm_as_a_judge\" )\n",
        "  else:\n",
        "    answerQuestion( question , prompt_type=\"specific\")"
      ],
      "metadata": {
        "id": "RyWppBJ4Zahf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚úçÔ∏è **Reflection**\n",
        "\n",
        "Please use the text cell below to answer the following question:\n",
        "\n",
        "What changes did you observe after applying the type-specific formatting prompts?\n",
        "Were there any challenges or difficulties in designing these prompts?"
      ],
      "metadata": {
        "id": "XawaZHLxcwAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your Answer**\n",
        "\n",
        "...\n",
        "\n"
      ],
      "metadata": {
        "id": "i2cQK223cwAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Engineering 2: LLM as a Judge"
      ],
      "metadata": {
        "id": "LPPbqYATUnQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Formatting helps us get better answers from LLMs, but evaluating those answers is equally important. The clearer the instructions, the more stable and reliable the LLM's scores will be.\n",
        "\n",
        "Looking at the prompt and the evaluation scores above, can you **revise the judge prompt and/or task prompt** to improve the LLM-as-a-judge's performance?"
      ],
      "metadata": {
        "id": "1ZoZyS2Hb8tN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Change the prompts! This is copied from above.\n",
        "\n",
        "JUDGE_PROMPT = \"\"\"# Instruction\n",
        "You will be given a question, gold answer, and system answer.\n",
        "Your task is to provide a 'total rating' scoring\n",
        "how well the system answer matches the gold answer for the question.\n",
        "Give your answer as an integer on a scale of 0 to 5, where\n",
        "0 means that the system answer does not match the gold answer at all,\n",
        "and 5 means that the system answer matches the gold answer.\n",
        "\n",
        "Provide your feedback as follows:\n",
        "\n",
        "# Feedback\n",
        "Rational: (your thinking process)\n",
        "Total rating: (your rating, as an integer from 0 to 5)\"\"\"\n",
        "\n",
        "TASK_PROMPT = \"\"\"# Task\n",
        "Now here are the question and answer.\n",
        "Question: {question}\n",
        "Gold Answer: {gold_answer}\n",
        "System Answer: {system_answer}\n",
        "\n",
        "# Feedback\n",
        "Rational: \"\"\""
      ],
      "metadata": {
        "id": "NtNXBrZRb8tP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üìã**Results**"
      ],
      "metadata": {
        "id": "JWUAYzYCcV6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change this cell if needed. The output should include the results of questions that use LLM as a judge.\n",
        "\n",
        "use_llm_as_a_judge = [] # TODO: Choose suitable evaluation type for each question.\n",
        "for i, question in enumerate([q1, q2, q3, q4, q5, q6, q7, q8, q9, q10]):\n",
        "  if i not in use_llm_as_a_judge: continue\n",
        "  print(f\"---------------- Question {i+1} ----------------\")\n",
        "  answerQuestion( question , prompt_type=\"specific\" , evaluation_type=\"llm_as_a_judge\" )"
      ],
      "metadata": {
        "id": "Azt782JlcV6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚úçÔ∏è **Reflection**\n",
        "\n",
        "Please use the text cell below to answer the following question:\n",
        "\n",
        "What observations guided your prompt design for the LLM-as-a-judge method?\n",
        "Did the prompts work as you expected?\n",
        "Were there any challenges in designing these prompts?"
      ],
      "metadata": {
        "id": "icWZZVgPdKHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your Answer**\n",
        "\n",
        "...\n",
        "\n"
      ],
      "metadata": {
        "id": "TrvuhrKedKHW"
      }
    }
  ]
}